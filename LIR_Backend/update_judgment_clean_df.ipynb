{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jSClkVQhHrXu"
      },
      "source": [
        "# Update irish_judgment_HC-CA-SC_refined_clean_df\n",
        "\n",
        "Before creating the SBert and law2Vec embeddings, i wanted to ensure that the judgments dataframe I using contained no NaN values as this could later cause corruption in my data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrPIx06spOfM",
        "outputId": "62762ee6-afa2-4a5f-d6fb-0fcb22fa76a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=e6e9f9ef5effabee3931f495e7a43e839744f2dc221219377ffe7d9b0b0023f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizer\n",
            "  Downloading tokenizer-3.4.2-py2.py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-3.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting s3fs\n",
            "  Downloading s3fs-2023.5.0-py3-none-any.whl (28 kB)\n",
            "Collecting aiobotocore~=2.5.0 (from s3fs)\n",
            "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.7/72.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec==2023.5.0 (from s3fs)\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from s3fs)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.29.77,>=1.29.76 (from aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading botocore-1.29.76-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore~=2.5.0->s3fs) (1.14.1)\n",
            "Collecting aioitertools>=0.5.1 (from aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.26.15)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.77,>=1.29.76->aiobotocore~=2.5.0->s3fs) (1.16.0)\n",
            "Installing collected packages: multidict, jmespath, fsspec, frozenlist, async-timeout, aioitertools, yarl, botocore, aiosignal, aiohttp, aiobotocore, s3fs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "Successfully installed aiobotocore-2.5.0 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 botocore-1.29.76 frozenlist-1.3.3 fsspec-2023.5.0 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.5.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.136-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.136 (from boto3)\n",
            "  Downloading botocore-1.29.136-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.136->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.136->boto3) (1.26.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.136->boto3) (1.16.0)\n",
            "Installing collected packages: botocore, s3transfer, boto3\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.29.76\n",
            "    Uninstalling botocore-1.29.76:\n",
            "      Successfully uninstalled botocore-1.29.76\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.29.136 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed boto3-1.26.136 botocore-1.29.136 s3transfer-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install s3fs\n",
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LkPuh3RDpOc2"
      },
      "outputs": [],
      "source": [
        "# standard library imports\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "# related third party imports\n",
        "import pandas as pd\n",
        "\n",
        "# imports for accessing s3 bucket\n",
        "import s3fs\n",
        "import boto3\n",
        "from getpass import getpass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bWYPFAxpOaR",
        "outputId": "b262dbba-5cc7-4277-9bb9-55b954d18298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input AWS access key ID:\n",
            "··········\n",
            "Input AWS secret access key:\n",
            "··········\n"
          ]
        }
      ],
      "source": [
        "# input private aws credentials if using Google Colab\n",
        "print('Input AWS access key ID:')\n",
        "aws_access_key_id = getpass()\n",
        "print('Input AWS secret access key:')\n",
        "aws_secret_access_key = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfU50DADpOFY",
        "outputId": "cb52a1c7-6d3d-4858-b002-f8c36e103a27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "s3.ObjectSummary(bucket_name='legal-research-thesis-data', key='SBert_embeddings_mpnet.pkl')\n",
            "s3.ObjectSummary(bucket_name='legal-research-thesis-data', key='irish_judgment_HC-CA-SC_refined_clean_df.csv')\n",
            "s3.ObjectSummary(bucket_name='legal-research-thesis-data', key='irish_judgment_HC-CA-SC_refined_clean_df_model_training.csv')\n",
            "s3.ObjectSummary(bucket_name='legal-research-thesis-data', key='irish_law2vec_embeddings.pkl')\n",
            "s3.ObjectSummary(bucket_name='legal-research-thesis-data', key='irish_law2vec_model.txt')\n"
          ]
        }
      ],
      "source": [
        "# this cell is for s3 bucket access when using Google Colab\n",
        "\n",
        "# enter authentication credentials\n",
        "s3 = boto3.resource('s3', aws_access_key_id = aws_access_key_id, \n",
        "                          aws_secret_access_key = aws_secret_access_key)\n",
        "\n",
        "# define bucket & file\n",
        "my_bucket = s3.Bucket('legal-research-thesis-data')\n",
        "\n",
        "# list bucket objects\n",
        "for my_bucket_object in my_bucket.objects.all():\n",
        "    print(my_bucket_object)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klKSjHtspN_O",
        "outputId": "560eb111-778c-41d4-f3ef-6da3ffe7c9bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17933\n",
            "   judgment_id neutral_citation  \\\n",
            "0            0  [2020] IEHC 628   \n",
            "1            1   [2015] IESC 72   \n",
            "2            2  [2013] IEHC 536   \n",
            "3            3  [1997] IEHC 133   \n",
            "4            4  [2019] IEHC 230   \n",
            "\n",
            "                                      judgment_title judgment_date  \\\n",
            "0  TMT Digital centre Limited & anor  v  Grehan &...    2020-11-27   \n",
            "1                  Fingal County Council  v  Kennedy    2015-07-31   \n",
            "2      S.O & anor  v  Refugee Appeals Tribunal & ors    2013-11-01   \n",
            "3                                  D.P.P. v. D. (J.)    1997-07-29   \n",
            "4  X (a minor)  v  The Board of Management of Sch...    2019-03-29   \n",
            "\n",
            "      court_name   judgment_by judgment_status  \\\n",
            "0     High Court    Twomey J.         Approved   \n",
            "1  Supreme Court    Laffoy J.         Approved   \n",
            "2     High Court     Clark J.         Approved   \n",
            "3     High Court       No data         No data   \n",
            "4     High Court   Barrett J.         Approved   \n",
            "\n",
            "                                            judgment  \\\n",
            "0  \\n[2020] IEHC 628\\nTHE HIGH COURT\\n[2019 No. 4...   \n",
            "1     THE SUPREME COURT  [Appeal No. 322/13] Hard...   \n",
            "2   Neutral Citation: [2013] IEHC 536   THE HIGH ...   \n",
            "3   CENTRAL CRIMINAL COURT   Bill No. C.C. 0011 o...   \n",
            "4   THE HIGH COURT 2019 No. 83 JR  Between:  X (A...   \n",
            "\n",
            "                                      clean_judgment  \\\n",
            "0  2020 iehc 628 high court 2019 4318 p tmt digit...   \n",
            "1  supreme court appeal hardiman mckechnie clarke...   \n",
            "2  neutral citation 2013 iehc 536 high court judi...   \n",
            "3  central criminal court bill 0011 1977 plaintif...   \n",
            "4  high court 2019 83 jr x minor sue father next ...   \n",
            "\n",
            "                                        judgment_url  \n",
            "0  https://courts.ie/acc/alfresco/95020573-cc98-4...  \n",
            "1  https://courts.ie/acc/alfresco/d685aed4-96cd-4...  \n",
            "2  https://courts.ie/acc/alfresco/6a8e82f3-727d-4...  \n",
            "3  https://www.bailii.org/ie/cases/IEHC/1997/133....  \n",
            "4  https://courts.ie/acc/alfresco/49e19a0f-8020-4...  \n"
          ]
        }
      ],
      "source": [
        "# read file from s3 if using Google Colab\n",
        "s3 = boto3.client('s3', aws_access_key_id = aws_access_key_id, \n",
        "                          aws_secret_access_key = aws_secret_access_key) \n",
        "\n",
        "# define bucket & object\n",
        "my_bucket = 'legal-research-thesis-data'\n",
        "judgment_object_clean = s3.get_object(Bucket = my_bucket, Key = 'irish_judgment_HC-CA-SC_refined_clean_df.csv') \n",
        "\n",
        "# read csv file from s3 into dataframes\n",
        "judgments_clean_df =pd.read_csv(judgment_object_clean['Body'])\n",
        "\n",
        "print(len(judgments_clean_df))\n",
        "\n",
        "print(judgments_clean_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TVNTBqOwphed"
      },
      "outputs": [],
      "source": [
        "# remove any NaN rows from judgments_clean_df\n",
        "judgments_clean_df = judgments_clean_df[judgments_clean_df['clean_judgment'].notnull()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXbbFwPXqw_e"
      },
      "source": [
        "Once the NaN rows had been removed from the judgments_clean_df dataframe, the index was reset in order to avoid discrepancies between the IDs for the SBert embeddings and law2Vec embeddings. As can be seen from the print statement in the cell below, there are now 17917 rows as opposed to 17933."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLoI8_IWpkOH",
        "outputId": "b3101d9f-68e0-49bd-fdf7-727edec483f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17917\n"
          ]
        }
      ],
      "source": [
        "# Reset Index/Judgement IDs as rows we're removed\n",
        "judgments_clean_df=judgments_clean_df.reset_index(drop=True)\n",
        "judgments_clean_df['judgment_id']=judgments_clean_df.index\n",
        "print(len(judgments_clean_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CVg0Pw-ry4L"
      },
      "source": [
        "Finally, the updated dataframe is saved from Google Colab. It is later added to the legal-research-thesis-data bucket manually. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Nw6HQW0gqP-G",
        "outputId": "97639bc4-eda0-48ac-a9a6-4c848ae29d8d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_3bce072f-bf16-490b-bc7f-abc8f3ac45c5\", \"irish_judgment_HC-CA-SC_refined_clean_df_v2.csv\", 1109651586)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# save embeddings from google colab\n",
        "from google.colab import files\n",
        "\n",
        "# write embeddings to csv file\n",
        "with open('irish_judgment_HC-CA-SC_refined_clean_df_v2.csv', 'wb') as f:\n",
        "   judgments_clean_df.to_csv('irish_judgment_HC-CA-SC_refined_clean_df_v2.csv')\n",
        "\n",
        "# download file locally\n",
        "files.download('irish_judgment_HC-CA-SC_refined_clean_df_v2.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7f2l8UC0TNY"
      },
      "source": [
        "To verify that there was no corruption to the DataFrame during transfer to s3, the updated DataFrame was downloaded. As can be seen, there are still 17917 rows in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqKmQhebyx7D",
        "outputId": "888f7d9b-d39d-4253-d362-c8aa529b89ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17917\n",
            "   Unnamed: 0  judgment_id neutral_citation  \\\n",
            "0           0            0  [2020] IEHC 628   \n",
            "1           1            1   [2015] IESC 72   \n",
            "2           2            2  [2013] IEHC 536   \n",
            "3           3            3  [1997] IEHC 133   \n",
            "4           4            4  [2019] IEHC 230   \n",
            "\n",
            "                                      judgment_title judgment_date  \\\n",
            "0  TMT Digital centre Limited & anor  v  Grehan &...    2020-11-27   \n",
            "1                  Fingal County Council  v  Kennedy    2015-07-31   \n",
            "2      S.O & anor  v  Refugee Appeals Tribunal & ors    2013-11-01   \n",
            "3                                  D.P.P. v. D. (J.)    1997-07-29   \n",
            "4  X (a minor)  v  The Board of Management of Sch...    2019-03-29   \n",
            "\n",
            "      court_name   judgment_by judgment_status  \\\n",
            "0     High Court    Twomey J.         Approved   \n",
            "1  Supreme Court    Laffoy J.         Approved   \n",
            "2     High Court     Clark J.         Approved   \n",
            "3     High Court       No data         No data   \n",
            "4     High Court   Barrett J.         Approved   \n",
            "\n",
            "                                            judgment  \\\n",
            "0  \\n[2020] IEHC 628\\nTHE HIGH COURT\\n[2019 No. 4...   \n",
            "1     THE SUPREME COURT  [Appeal No. 322/13] Hard...   \n",
            "2   Neutral Citation: [2013] IEHC 536   THE HIGH ...   \n",
            "3   CENTRAL CRIMINAL COURT   Bill No. C.C. 0011 o...   \n",
            "4   THE HIGH COURT 2019 No. 83 JR  Between:  X (A...   \n",
            "\n",
            "                                      clean_judgment  \\\n",
            "0  2020 iehc 628 high court 2019 4318 p tmt digit...   \n",
            "1  supreme court appeal hardiman mckechnie clarke...   \n",
            "2  neutral citation 2013 iehc 536 high court judi...   \n",
            "3  central criminal court bill 0011 1977 plaintif...   \n",
            "4  high court 2019 83 jr x minor sue father next ...   \n",
            "\n",
            "                                        judgment_url  \n",
            "0  https://courts.ie/acc/alfresco/95020573-cc98-4...  \n",
            "1  https://courts.ie/acc/alfresco/d685aed4-96cd-4...  \n",
            "2  https://courts.ie/acc/alfresco/6a8e82f3-727d-4...  \n",
            "3  https://www.bailii.org/ie/cases/IEHC/1997/133....  \n",
            "4  https://courts.ie/acc/alfresco/49e19a0f-8020-4...  \n"
          ]
        }
      ],
      "source": [
        "# read file from s3 if using Google Colab\n",
        "s3 = boto3.client('s3', aws_access_key_id = aws_access_key_id, \n",
        "                          aws_secret_access_key = aws_secret_access_key) \n",
        "\n",
        "# define bucket & object\n",
        "my_bucket = 'legal-research-thesis-data'\n",
        "judgment_object_clean = s3.get_object(Bucket = my_bucket, Key = 'irish_judgment_HC-CA-SC_refined_clean_df_v2.csv') \n",
        "\n",
        "# read csv file from s3 into dataframes\n",
        "judgments_clean_df =pd.read_csv(judgment_object_clean['Body'])\n",
        "\n",
        "print(len(judgments_clean_df))\n",
        "\n",
        "print(judgments_clean_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR5FP7vd0NES",
        "outputId": "491cb647-5761-4dc9-c148-bf1d9049fce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17917\n"
          ]
        }
      ],
      "source": [
        "print(len(judgments_clean_df))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
